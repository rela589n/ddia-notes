![[Transactions - Poor Performance?]]

Things that can go wrong:
- **database crashes** at the mid of write operation;
- **application crashes** at the mid of writes;
- **network interrupts** between app and db or between db nodes;
- **clients overwrite changes** of each other;
- **partially updated data is read**;
- other **race condition bugs**.

**Transaction** - sequence of **reads and writes** which make up a **logical unit** and should be **comitted atomically**. It provides **safety guarantees** so that we can **ignore** some **error scenarios** and **concurrency issues**.

Some apps don't need transactions, and for better performance transactional guarantees may be weakened or disabled completely. 

## The Slippery Concept of a Transaction

![[NoSQL - no Transactions#^21bd59]]

### The Meaning of ACID

**ACID** - stands for **Atimicity**, **Consistency**, **Isolation**, **Durability**.

Systems that do **not meet ACID** are sometimes called **BASE** - vague acronym stands for **Basically Available**, **Soft state**, **Eventual consistency**.

#### Atomicity

**Atomic** - something that can't be broken down into parts.
**Atomicity** - the **ability to abort the transaction** partway in case if any error occurs.

#### Consistency

**Consistency** - the property of database when it **follows application invariants**. It is not always enforced by database itself, though some **constraints** may be defined (**foreign keys**, **checks**, **unique**, etc.), but usually it is **up to application to enforce** consistency rules.

If each statement of transaction follows app consistency rules, then after commit database will be in the consistent state.

#### Isolation

**Isolation** - database guarantee that when **multiple transactions access the same rows**, they are **isolated** from each other (e.g. not reading dirty writes). Typical example is the concurrent counter modification.

#### Durability

**Durability** - database guarantee that **once transaction has committed**, it will hold value so that **we can read the data later** regardless of crashes.

In the **replicated databases durability** means that **write** was successfully **processed by W nodes**.

##### Replication and Durability

There's no silver bullet for providing a full durability:
- if **machine fails**, we would need to either repair the machine or to **move the disk to another** machine;
- if **code has a bug**, then it will lead to **correlated fault** and no writes will be persisted;
- if using **async replication**, the leader may fail and **recent writes will be lost**;
- if **power is cut off**, **SSD** may not save the data;
- after crashes **files on disk may be corrupted**;
- data may **get corrupted over time** without it being detected;
- within first 4 years, **SSD**s start to have **bad blocks**;
- **HDD**s have high rate of **complete failure**;
- if **SSD is disconnected from power**, it may start **losing the data** within few weeks.

### Single-Object and Multi-Object operations

**Multi-object transactions** - transactions, which **modify multiple objects** (rows, records, documents). They are especially necessary **if data must be kept in sync** (say, denormalized counter for some query).

The **violation of an isolation** is when on the **halfway of the transaction**, some **other client reads just written data**. **Reading of not committed writes** is known as **dirty read**.

To group statements, `START TRANSACTION` and `COMMIT` commands are used. It's possible that **client sent commit** operation, but **TCP connection was interrupted** at this time. Client **can't know** whether transaction was **committed or not**.

#### Single-Object Writes

**Single-object writes** may face **issues the mid of large content upload**:
- the **network is interrupted** or **power fails** - not to store partially inserted/replaced value (**_Atomicity_**). It is implemented using **WAL**;
- **another client** tries to **read** the **same record** - not to show spliced up record (**_Isolation_**). It is implemented using **locks on objects** - only one thread has an access;

> How does it work with transactions? If we modify content in current transation and in the mid of upload server interrupts. How does other clients not see corrupted data? 
> The answer is that MVCC updates are equivalent to delete and create operations. Hence, old value is kept not being overwritten. New value will be abandoned because write didn't commit.

DB support **atomic operations** (like **increments**) to avoid **read-modify-write** cycle, which is **prone to the issue of lost updates**.

**Compare-and-set** may be used to **prevent update** of the **concurrently modified record**.

#### The need for Multi-Object Transactions

**Multi-Object transactions** are **must-have** for most applications.
Conceptually nothing prevents us from distributed transactions implementation, though it is not supported now.

Issues in **not having multi-object transactions**:
1. when inserting **records which reference** one another using **foreign keys**, the data gets out of sync in the mid of writes;
2. since **document databases** encourage **denormalization**, the same **data** need to be **updated in multiple places**, this may get our data out of sync;
3. **secondary indexes** are separately standing storages, which are necessary to be **kept up-to-date with main data**. If actual record was written, but index was not yet updated, then query using this index will not read the record.

#### Handling Errors and Aborts

In **leaderless replication**, it is **up to application to recover** from errors.

The whole point of **transaction aborts** is to **allow safe retries**. Though, this is a pity that some ORM libraries does not retry at all and just throw away user input and show the error message.

Pitfalls of retried transactions:
- if **transaction was not ACK-ed** to client, but it was actually executed, then retry **may write data twice** unless app **deduplicate**s it;
- if transaction aborted because **DB is overloaded**, retry is **even  worse**. One may **limit the number of retries**, use **exponential backoff**, try to **handle** overload-related issues **separately**; 
- **retry won't help** to solve **non-recoverable issues** (**constraint violations**); **retry may help** for **transient issues** like deadlocks, failover, network interruptions, isolation violations;
- some **third-party systems** may **receive multiple (or none) messages** instead of one. **Two-phase commit** (**2PC**) may help to avoid it;
- after couple retries **client may fail** itself, which will mean not written data.

## Weak Isolation Levels

Race conditions arise when users access the same (or dependent) database rows. **Transaction isolation** was meant to **hide concurrency issues** from devs. **Serializable** level means that any **transactions** are executed **as if they were serial**, though it is rarely used because of **high performance penalty**.

### Read Comitted

**Read comitted** - isolation level that provides **no dirty reads** (selects will return **only what was committed**) and **no dirty writes** (writes will overwrite **only comitted data**) guarantees.

#### No dirty reads

**Dirty read** - when concurrent **transaction sees uncommitted writes**.

Reasons to **prevent dirty reads**:
- if some statements have **temporal coupling**, another transaction may see the **database in an invalid state** and take wrong decisions;
- if transation **rolls back**, and another client **depends on rolled back changes**, this will lead to mind-blowing consequences.

#### No dirty writes

**Dirty write** - when concurrent **transaction overwrites just written record**. Usually this is **solved by delaying the write** of a concurrent client until the end of the first transaction.

Reasons to **prevent dirty writes**:
- if some statements have **temporal coupling** for both transactions, second transaction may overwrite the value for one statement, while first transaction will overwrite the value for another statement of a second transaction, therefore **leading to the invalid state** (e.g. owner is set to Alice, while invoice is created for Bob);
- if some statements must **accumulate one on another** (like counter increments), some **updates may be lost** (e.g. incremented only once).

#### Implementing read committed

To **prevent dirty writes**, **row-level locks** are used. To write the record, database locks row, and **keeps the lock until the end of transaction**. Any other transactions will wait until the lock is released.

To **prevent dirty reads**, there are two approaches:
1. (_not used_) briefly **acquire the write lock before reading** the row and **release it right away** after reading. This has the performance penalty since incoming reads wait for some write transactions to commit;
2. database may **keep both old and new values**. While concurrent transaction is **not committed yet**, **old value is returned**. **After commit** transactions **switch to the new value**.

### Snapshot Isolation and Repeatable Read

?? How does it all work if we don't wrap reads into a transaction

**Non-reepeatable read** (**Read skew**) - if **during single transaction** the **same record is queried**, it may be **in different states**. It may be even more subtle when two **rows are bound one to another**, and we read **first before the change**, and **second after the change** 

For instance, money transfer between accounts - first account looks like if it has never received the money, while second is as if money were subtracted from it. This makes user think that money vanished into the air.

The **read skew is not acceptable**:
- **for backups** - if restored, the **inconsistencies will become permanent** (like lost money on the balance);
- for **integrity checks**, **analytic queries** - usually scan over a large set of database records; seeing **database in different points of time** will result in **nonsensical results**.

**Snapshot isolation** (**repeatable read**) - each **transaction sees** rows in the sate **committed before the beginning** of the transaction.

#### Implementing Snapshot Isolation

The key principle of snapshot isolation is that **reads never block writes** and **writes never block reads**.

**MVCC** (Multi-version concurrency control) - snapshot isolation technique, by which **writes use locks** the same way **as in read committed** isolation, and to serve the reads, records **besides of the data hold the id of transactions** (txid) which wrote this data. Id txid is an incremental integer.

The records contain `created_by` and `deleted_by` fields for each row. When the row is updated it is the same as previous version was deleted and new created, iow `update=delete+create`.

#### Visibility rules for observing a consistent snapshot

**Consistent snapshot** is built following way:
- writes of **running transactions are ignored** (list of active transactions is built beforehand at the beginning of current transaction);
- writes made by **rolled back transactions are ignored**;
- writes made by **transactions with greater txid are ignored** - started after current one;
- all other writes are visible to queries.

#### Indexes and snapshot isolation

Indexation highly depends on internal implementation.
- the **BTree index** may **point to all versions of rows** and query executor will filter out not visible rows based on txid;
- the **copy-on-write** approach may be used - every write creates a new root of BTree, hence making consistent snapshot, therefore no need to filter the actual rows.

#### Repeatable read and naming confusion

**Snapshot isolation** is called **serializable** in Oracle and **repeatable read** in PostgreSQL and MySQL. In IBM DB2 **repeatable read** refers to **serializability**.

### Preventing Lost Updates

**Lost update** - when one **concurrent transaction clobbers modifications of another** transaction. It happens in **read-modify-write** cycle:
- counter increment, balance changes;
- local change in a complex value (say, adding json field);
- full update of the object (say, two users modify some entity in AP).

#### Atomic write operations

In some cases **instead of read-modify-write** (plain assignment, rewrite) the **modification operation** may be **applied** to the value which will lead to the final expected state. For instance, increment is applied `SET val = val + 1`, for json modification `jsonb_set` may be used.

#### Explicit locking

When logic can't be sensibly implemented via **atomic write operations**, then during `select` statement the `for update` may be specified. This will lock all rows returned by the query until current transaction commits.

#### Automatically detecting lost updates

PostgreSQL, Oracle, Sql Server support automatic lost update detection when running in **snapshot isolation** level. MySQL doesn't detect lost updates 
automatically.

If there were **concurrent transaction**, which **modified the same row as we**, DB will **roll back current transaction** and throw an error.

#### Compare-and-set

If **DB doesn't provide a transactions**, we may detect lost read from our side using **compare-and-set** approach.

Update statement will add **where conditions to check old data state**, therefore update will not apply if old state was changed. Therefore, if no rows were updated, then operation should be retried.

It may not be safe, and depends on internals of DB. If DB compares where conditions on old snapshot, but runs the update on already updated data, this may be an issue.

#### Conflict Resolution and Replication

The **only working way** with no overhead is by means of **atomic operations**, especially which are **commutative** (like counter increment, addition etc).

**Neither compare-and-set nor explicit locks** can help to **prevent lost updates** in replicated systems (**multi-leader, leaderless**). Therefore, it is necessary to implement **conflict resolution logic** as written previously in [[5. Replication#Handling write conflicts|Handling write conflicts]].

### Write Skew and Phantoms

Example: there should always be at least one doctor for every slot. Two doctors request sick leave at the same time for the same slot. Query checks that there are two doctors for this slot and both go off.

#### Characterizing write skew

**Write skew** - generalization of lost update problem (lost update happens to the same record, while write skew is about separate records). It is the anomaly when two **parallel transactions run read queries**, which the further **update logic depend on**, leading to **logic execute in both cases** instead of one.

**Write skew** can happen **if write depends on presence/absence of some records**, while this **write may insert new or modify this set of records** on it's own. Hence, if two transactions read the value, first makes the modification, then second is not aware that subset has changed and write skew happens.

Solutions:
- **automatic** write skew prevention requires **serializable isolation level**;
- **DB constraints** that enforce consistency (e.g. PostgreSQL Exclude checks);
- **explicit lock** on rows the transaction depend on.

#### More examples of write skew

- **meeting room booking system** - two users may try to book the same room at the same time;
- **multiplayer game** - two players may try to put the figures on the same position;
- **user registration** (unique email) - both transactions check that username doesn't exist and both try to insert a new user;
- **double-spending** - two transactions check that there's enough money and both spend so that balance becomes negative.

#### Phantoms causing write skew

The write skew pattern is following:
1. select **query** searches for rows that **match some condition**;
2. depending on the result of query, **app decides what to do** (either report an error or make the write);
3. if condition from step 2 is satisfied, app **makes a write and commits** the transaction. The committed **write affects the result of query** from step 1.

**Phantom** - the effect when the **write in one transaction changes the query result in another** after this query was executed.

#### Materializing conflicts

**Materializing conflicts** - **last resort** approach to **prevent write skew** when **write depends on absence of rows**. The **separate table** is introduced, which is not used for storing the actual information, but rather it is **used as a collection of locks**. **Read query will lock respective rows** in this table and other transaction which overlaps with current one will have to wait.

Considering meeting room booking example, table will contain data with 15 minutes time slots for every room for next 6 months. The transaction will lock overlapping rows in this table.

This approach is a **Last Resort**, usually **Serializability should be used instead**.

## Serializability

**Serializable** isolation level guarantees that even though **transactions may run in parallel**, the final **result** would be **as if they ran serially**.

**Techniques to implement** serializability:
- **literally execute** transactions **serially**;
- **two-phase locking** (2PL);
- **serializale snapshot isolation** (SSI).

### Actual Serial Execution

#VoltDB , #Redis, #Datomic, #H-Store

Reasons to rethink actual serial execution:
- RAM is cheap, some **DBs keep everything in memory**;
- In most cases **OLTP transactions are short**, while OLAP are usually readonly and can operate on snapshot isolation level.

#### Encapsulating transactions in stored procedures

The **interactive style** of transactions leverages **network** a lot (requests, responses). If we allow only one transaction to run in one point of time, this would lead to **dreadful performance**.

Another approach is to use **stored procedure** and make the **DB aware of all steps of the transaction** beforehand. Even though transactions are executed one by one they don't wait for the application to submit the next query, hence **performance is NOT limited by network** anymore, also we don't have the concurrency control overhead and locks. If  the **dataset is completely in memory**, we'll have **no disk IO overhead**. This allows us to have pretty good performance in single-threaded environments.

#### Pros and cons of stored procedures

**Bad reputation** of stored procedures:
- each vendor uses it's **own language** to write them. This is not a general-purpose language, no libraries are available;
- **maintenance is harder** - hard to debug, tricky to test, need to deploy, harder to maintain in VCS, monitoring and metrics collection is not possible;
- **instance performance** - multiple app instances may connect to the single database. While app scaling is relatively simpler, DB scaling is much more harder. Therefore, **poorly written** code in **procedure** leads to **worse outcome** compared to the same poor app code.

Though, **some DBs** now **use general-purpose languages** (VoltDB uses Java and Groovy, Datomic uses Java and Clojure, Redis uses Lua).

#### Partitioning

For apps with **high write throughput** a **single-threaded transactions** processor may be a **serious bottleneck**.

Write throughput may be **scaled to multiple CPUs** if it is possible for application to partition the data in the way so that **transactions operate within the partition boundaries**.

In VoltDB it is also possible to run **cross-partition transactions**. Though, it's write throughput is quite limited and can't be scaled, as well as it is a lot **slower compared to single-partition transaction**.

#### Summary of serial execution

Actual serial execution is viable with remarks:
- every **transaction must be lightning-fast** (no network communication, everything through stored procedure);
- **active data set must fit in memory**. Some rarely-used data may be backed up on disk, but when it's necessary again, **anti-caching** approach should be used - roll back transaction, fetch data in memory, restart the transaction;
- **write throughput** must be **low enough** to be handled by single CPU, **otherwise partitioning is necessary** to be used with the requirement of no cross-partition transaction;
- **cross-partition transactions are possible** as well, though require DB coordination and are substantially slower compared to single-partition tansactions.

### Two-Phase Locking (2PL)

**Two-phase locking** (**2PL**) - approach to provide serializability, which **protects from all kinds of race conditions** including lost updates and write skew. It no longer follow **reads never block writes** and **writes never block reads**. 

Instead reads block writes and vice versa:
- if transaction **A has read object**, and transaction **B wants to write it**, B will have to **wait for A to finish** (no unexpected changes to read objects);
- if transaction **A has wrote object**, and transaction **B wants to read it**, B will have to wait **for A to finish** (reading old versions of object is not acceptable).

#### Implementation of 2PL

Used by MySQL, SQL Server, DB2

**Shared lock** - does not prevent other transactions from taking this lock on the same rows, but does **prevent taking exclusive lock**.
**Exclusive lock** - **prevents any other transaction from locking** with either Shared or Exclusive lock on the same rows.

Locks are used following way:
- when transaction **reads something**, it acquires **shared lock** on these rows;
- when transaction **writes something**, it acquires **exclusive lock** on these rows;
- if transaction **first reads then writes**, then **shared lock is promoted to exclusive lock** on the modified rows;
- locks are hold until the end of transaction.

**Two-phase locking** means:
- **during transaction** execution, **locks are acquired** (first phase);
- at the **end of transaction** all **locks are released** (second phase).

In some situations **deadlock** may arise, then **one transaction is aborted** (it should be retried by the application), while **other continues** execution.

#### Performance of 2PL

**Concurrency is reduced**. One single small transaction **may wait for a dozen transactions** to complete. It takes only one single large transaction to grind system to halt.
Because of it, **latency is unstable**. At high percentiles 2PL DBs are very slow.
**Locks** itself **introduce overhead**.
**Deadlocks** in 2PL are quite frequent. This brings an **overhead of retrial**.

#### Predicate locks

2PL with no predicate locks is vulnerable to new inserts (phantoms), which will cause write skew. Since non-existing data isn't locked, nothing prevents it from modification of other transaction query result once committed.

**Predicate lock** - **shared lock** on the query **search criteria**. Key idea is that it **applies to objects** which do **not yet exist** in the database. It makes 2PL serializable.

Combined with 2PL predicate locks:
- when data is **read**, **predicate lock is acquired first**. If another transaction holds exclusive lock on any object which matches the predicate, current transaction will have to **wait until writing transaction commits**;
- when data is **modified** (insert, update, delete), the transaction has to **check if** either old or new **value matches any existing predicate locks**. If so, current transaction will have to **wait until predicate lock is released**.

#### Index-range locks

**Index-range locking** (**next key locking**) - **shared lock** is attached **to index entries** during the search. When insert/update/delete trigger **update of index**, transaction will have to **wait for shared lock** to be released. This approach is used in most 2PL implementations **instead of predicate locks**.

This approach is **not that precise** as **predicate locks** are (the index may leverage only one field, while query uses multiple), but it is a good **performance compromise**, since checking all predicate locks during the update takes a lot of time, while **index update is necessary anyway** (if there's index, if no - shared lock is taken on whole table).

> ?? how does index-range locking work when update is triggered by primary key. If the updated field is not used in index, write is supposed to acquire lock anyway. How it will do? Will it scan all existing indexes? Maybe index is only necessary for create and delete, while for update the locks on actual rows are used.

### Serializable Snapshot Isolation (SSI)

SSI has small performance penalty compared to snapshot isolation.

#### Pessimistic vs Optimistic concurrency control

**Pessimistic concurrency control** is essentially what 2PL is. If **anything could possibly go wrong**, we'd **wait until situation is safe** again and only then continue execution. Actual serial execution is pessimistic to the extreme - lock is held on entire database.

**Optimistic concurrency control** - transactions continue operate normally and **hope that everything will turn all right**. If not, some of the transactions are aborted. It operates **badly with high contention rate**, because a lot of may transactions get aborted.

**Serializable Snapshot Isolation (SSI)** - optimistic concurrency control method, **based on snapshot isolation level**, which **adds** logic of **serialization anomalies detection** at the transaction commit time, and if there are any, **aborts some transactions**.

#### Decisions based on outdated premise

The transaction queries may have causal dependency (e.g. writes depend on read data).

**Outdated Premise** - statement that was true at the beginning of transaction, but no longer true at the commit time.

Cases to **detect** possibly **outdated premises**:
- **reading stale MVCC objects** (there were uncommitted writes before read);
- **writes that affect premise** (the write was published after read).

If on commit time there are any outdated premises related to current transaction read queries, it is aborted.

#### Detecting stale MVCC reads

When transaction **reads object** from the MVCC table, it may find out that **some not committed writes** from other transactions **are present**. It will **keep track** of these writes **until the commit**. If during the commit it turns out that **writes were committed**, current **transaction is aborted**.

#### Detecting writes that affect prior reads

When transaction **reads the data**, this information is **tracked in index** entries similarly to [[7. Transactions#Index-range locks|Index-range locks]], hence we can know which data is read by which transaction.

When transaction **writes the data**, it looks at the indexes to **check which transactions have read matching rows**. Then, current transaction **notifies** transactions **about new outdated premise**.

#### Performance of Serializable Snapshot Isolation

As with **Snapshot Isolation**, readers don't block writers and writers don't block readers, therefore **queries latency is predictable**.

The overall **performance** of SSI highly **depends on rate of aborts**. It in turn, **depends** on internal implementation of **transactions activity bookkeeping**. The more **precise** it is, the **less chance of** unnecessary **abort** is, but it **adds its own overhead**. 

As a rule of thumb, **SSI transactions should not be long-running**, because in case of abort it would take some time to retry. SSI is less sensitive to slow transactions compared to 2PL or serial execution.

## Summary

Transactions are the **abstraction**, which allows us to **pretend that some faults do not exist**. A lot of errors come to simple transaction abort.

**Without transactions** process, power, disk, network, concurrency **issues lead to inconsistencies** in the DB. For instance, keeping up to date denormalized data.

Concurency control **isolation levels**:
- **read committed**;
- **snapshot isolation**;
- **serializable**.

Race conditions, which different isolation levels prevent:
- **dirty reads** - transaction A reads uncommitted changes of B. Prevented in **^read commited**.
- **dirty writes** - transaction A overwrites not commited changes of transaction B. Always prevented.
- **read skew** (**nonrepeatable read**) - transaction sees **different parts of the database** in **different points of time**. This is prevented in **snapshot isolation** with **MVCC**.
- **lost updates** - two transactions use **read-modify-write** cycle leading to changes of one to be overwritten. Some DBs detect lost updates in **snapshot isolation**, while for others **explicit lock** to be used.
- **write skew** - transaction **reads something** (premise), **makes decision** based on it and **performs write**, which leads to **oudated premise** of another transaction. Only **snapshot isolation** prevents it.
- **phantom reads** - **transaction searches rows** matching the condition. **Another** transaction **performs the write**, which **affects the result set** of search. **Snapshot isolation** prevents it by means of **index-range locks** which may not be as fine-grained as necessary.

**Serializable isolation level** protects against all race conditions:
- **serial execution** - may be used if write throughput is low and each transaction is fast to execute;
- **two-phase locking** - uses locks, unreliable performance, first concurrent serializable isolation level implementation;
- **serializable snapshot isolation** - uses optimistic lock approach, transactions execute in parallel, on commit serializability is checked and if not satisfied transaction is aborted.

