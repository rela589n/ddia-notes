![[Transactions - Poor Performance?]]

Things that can go wrong:
- **database crashes** at the mid of write operation;
- **application crashes** at the mid of writes;
- **network interrupts** between app and db or between db nodes;
- **clients overwrite changes** of each other;
- **partially updated data is read**;
- other **race condition bugs**.

**Transaction** - sequence of **reads and writes** which make up a **logical unit** and should be **comitted atomically**. It provides **safety guarantees** so that we can **ignore** some **error scenarios** and **concurrency issues**.

Some apps don't need transactions, and for better performance transactional guarantees may be weakened or disabled completely. 

## The Slippery Concept of a Transaction

![[NoSQL - no Transactions#^21bd59]]

### The Meaning of ACID

**ACID** - stands for **Atimicity**, **Consistency**, **Isolation**, **Durability**.

Systems that do **not meet ACID** are sometimes called **BASE** - vague acronym stands for **Basically Available**, **Soft state**, **Eventual consistency**.

#### Atomicity

**Atomic** - something that can't be broken down into parts.
**Atomicity** - the **ability to abort the transaction** partway in case if any error occurs.

#### Consistency

**Consistency** - the property of database when it **follows application invariants**. It is not always enforced by database itself, though some **constraints** may be defined (**foreign keys**, **checks**, **unique**, etc.), but usually it is **up to application to enforce** consistency rules.

If each statement of transaction follows app consistency rules, then after commit database will be in the consistent state.

#### Isolation

**Isolation** - database guarantee that when **multiple transactions access the same rows**, they are **isolated** from each other (e.g. not reading dirty writes). Typical example is the concurrent counter modification.

#### Durability

**Durability** - database guarantee that **once transaction has committed**, it will hold value so that **we can read the data later** regardless of crashes.

In the **replicated databases durability** means that **write** was successfully **processed by W nodes**.

##### Replication and Durability

There's no silver bullet for providing a full durability:
- if **machine fails**, we would need to either repair the machine or to **move the disk to another** machine;
- if **code has a bug**, then it will lead to **correlated fault** and no writes will be persisted;
- if using **async replication**, the leader may fail and **recent writes will be lost**;
- if **power is cut off**, **SSD** may not save the data;
- after crashes **files on disk may be corrupted**;
- data may **get corrupted over time** without it being detected;
- within first 4 years, **SSD**s start to have **bad blocks**;
- **HDD**s have high rate of **complete failure**;
- if **SSD is disconnected from power**, it may start **losing the data** within few weeks.

### Single-Object and Multi-Object operations

**Multi-object transactions** - transactions, which **modify multiple objects** (rows, records, documents). They are especially necessary **if data must be kept in sync** (say, counter for some query).

The **violation of an isolation** is when on the **halfway of the transaction**, some **other client reads just written data**. **Reading of not committed writes** is known as **dirty read**.

To group statements, `START TRANSACTION` and `COMMIT` commands are used. It's possible that **client sent commit** operation, but **TCP connection was interrupted** at this time. Client **can't know** whether transaction was **committed or not**.

#### Single-Object Writes

**Single-object writes** may face **issues the mid of large content upload**:
- the **network is interrupted** or **power fails** - not to store partially inserted/replaced value (**_Atomicity_**). It is implemented using **WAL**;
- **another client** tries to **read** the **same record** - not to show spliced up record (**_Isolation_**). It is implemented using **locks on objects** - only one thread has an access;

?? How does it work with transactions?? If we modify content in current transation and in the mid of upload server interrupts. How does other clients not see corrupted data?

DB support **atomic operations** (like **increments**) to avoid **read-modify-write** cycle, which is **prone to the issue of lost updates**.

**Compare-and-set** may be used to **prevent update** of the **concurrently modified record**.

#### The need for Multi-Object Transactions

**Multi-Object transactions** are **must-have** for most applications.
Not supported now, conceptually nothing prevents us from distributed transactions implementation.

Issues in not having multi-object transactions:
1. when inserting **records which reference** one another using **foreign keys**, the data gets out of sync in the mid of writes;
2. since **document databases** encourage **denormalization**, the same **data** need to be **updated in multiple places**, this may get our data out of sync;
3. **secondary indexes** are separately standing storages, which are necessary to be **kept up-to-date with main data**. If actual record was written, but index was not yet updated, then query using this index will not read the record.

#### Handling Errors and Aborts

In **leaderless replication**, it is **up to application to recover** from errors.

The whole point of **transaction aborts** is to **allow safe retries**. Though, some ORM do not retry at all and just throw away user input and show the error message.

Pitfalls of retried transactions:
- if **transaction was not ACK-ed** to client, but it was actually executed, then retry **may write data twice** unless app **deduplicate**s it;
- if transaction aborted because **DB is overloaded**, retry is **even  worse**. One may **limit the number of retries**, use **exponential backoff**, try to **handle** overload-related issues **separately**; 
- **retry won't help** to solve **non-recoverable issues** (**constraint violations**); **retry may help** for **transient issues** like deadlocks, failover, network interruptions, isolation violations;
- some **third-party systems** may **receive multiple (or none) messages** instead of one. **Two-phase commit** (**2PC**) may help to avoid it;
- after couple retries **client may fail** itself, which will mean not written data.

## Weak Isolation Levels

Race conditions arise when users access the same database rows. **Transaction isolation** was meant to **hide concurrency issues** from devs. **Serializable** level means that any **transactions** are executed **as if they were serial**, though it is rarely used because of **high performance penalty**.

### Read Comitted

**Read comitted** - isolation level that provides **no dirty reads** (selects will return **only what was committed**) and **no dirty writes** (writes will overwrite **only comitted data**) guarantees.

#### No dirty reads

**Dirty read** - when concurrent **transaction sees uncommitted writes**.

Reasons to **prevent dirty reads**:
- if some statements have **temporal coupling**, another transaction may see the **database in an invalid state** and take wrong decisions;
- if transation **rolls back**, and another client **depends on rolled back changes**, this will lead to mind-blowing consequences.

#### No dirty writes

**Dirty write** - when concurrent **transaction overwrites just written record**. Usually this is **solved by delaying the write** of a concurrent client until the end of the first transaction.

Reasons to **prevent dirty writes**:
- if some statements have **temporal coupling** for both transactions, second transaction may overwrite the value for one statement, while first transaction will overwrite the value for another statement of a second transaction, therefore **leading to the invalid state** (e.g. owner is set to Alice, while invoice is created for Bob);
- if some statements must **accumulate one on another** (like counter increments), some **updates may be lost** (e.g. incremented only once).

#### Implementing read committed

To **prevent dirty writes**, **row-level locks** are used. To write the record, database locks row, and **keeps the lock until the end of transaction**. Any other transactions will wait until the lock is released.

To **prevent dirty reads**, there are two approaches:
1. (_not used_) briefly **acquire the write lock before reading** the row and **release it right away** after reading. This has the performance penalty since incoming reads wait for some write transactions to commit;
2. database may **keep both old and new values**. While concurrent transaction is **not committed yet**, **old value is returned**. **After commit** transactions **switch to the new value**.

### Snapshot Isolation and Repeatable Read

?? How does it all work if we don't wrap reads into a transaction

**Non-reepeatable read** (**Read skew**) - if **during single transaction** the **same record is queried**, it may be **in different states**. It may be even more subtle when two **rows are bound one to another**, and we read **first before the change**, and **second after the change** 

For instance, money transfer between accounts - first looks like if it has never received the money, while second is as if money were subtracted from it.

The **read skew is not acceptable**:
- **for backups** - if restored, the **inconsistencies will become permanent** (like lost money on the balance);
- for **integrity checks**, **analytic queries** - usually scan over a large set of database records; seeing **database in different points of time** will result in **nonsensical results**.

**Snapshot isolation** (**repeatable read**) - each **transaction sees** the data in the sate **committed before the beginning** of the transaction.

#### Implementing Snapshot Isolation

The key principle of snapshot isolation is that **reads never block writes** and **writes never block reads**.

**MVCC** (Multi-version concurrency control) - snapshot isolation technique, by which writes use locks the same way as in **read committed** isolation, and to serve the reads, records **besides of the data hold the id of transactions** (txid) which wrote this data. Id txid is an incremental integer.

The records contain `created_by` and `deleted_by` fields for each row. When the row is updated it is the same as previous version was deleted and new created, iow `update=delete+create`.

#### Visibility rules for observing a consistent snapshot

**Consistent snapshot** is built following way:
- writes of **running transactions are ignored** (list of active transactions is built beforehand at the beginning of current transaction);
- writes made by **rolled back transactions are ignored**;
- writes made by **transactions with greater txid are ignored** - started after current one;
- all other writes are visible to queries.

#### Indexes and snapshot isolation

Indexation highly depends on internal implementation.
- the **BTree index** may **point to all versions of rows** and query executor will filter out not visible rows based on txid;
- the **copy-on-write** approach may be used - every write creates a new root of BTree, hence making consistent snapshot, therefore no need to filter the actual rows.

#### Repeatable read and naming confusion

**Snapshot isolation** is called **serializable** in Oracle and **repeatable read** in PostgreSQL and MySQL. In IBM DB2 **repeatable read** refers to **serializability**.

### Preventing Lost Updates

**Lost update** - when one **concurrent transaction clobbers modifications of another** transaction. It happens in **read-modify-write** cycle:
- counter increment, balance changes;
- local change in a complex value (say, adding json field);
- full update of the object (say, two users modify some entity in AP).

#### Atomic write operations

In some cases **instead of read-modify-write** (plain assignment, rewrite) the **modification operation** may be **applied** to the value which will lead to the final expected state. For instance, increment is applied `SET val = val + 1`, for json modification `jsonb_set` may be used.

#### Explicit locking

When logic can't be sensibly implemented via **atomic write operations**, then during `select` statement the `for update` may be specified. This will lock all rows returned by the query until current transaction commits.

#### Automatically detecting lost updates

PostgreSQL, Oracle, Sql Server support automatic lost update detection when running in **snapshot isolation** level. MySQL doesn't detect lost updates 
automatically.

If there were **concurrent transaction**, which **modified the same row as we**, DB will **roll back current transaction** and throw an error.

#### Compare-and-set

If **DB doesn't provide a transactions**, we may detect lost read from our side using **compare-and-set** approach.

Update statement will add **where conditions to check old data state**, if old state was changed, update will not apply. Therefore, if no rows were updated, then operation should be retried.

It may not be safe, and depends on internals of DB. If DB compares where conditions on old snapshot, but runs the update on already updated data, this may be an issue.

#### Conflict Resolution and Replication

The **only working way** with no overhead is by means of **atomic operations**, especially which are **commutative** (like counter increment, addition etc).

**Neither compare-and-set nor explicit locks** can help to **prevent lost updates** in replicated systems (**multi-leader, leaderless**). Therefore, it is necessary to implement **conflict resolution logic** as written previously in [[5. Replication#Handling write conflicts|Handling write conflicts]].

### Write Skew and Phantoms

Example: there should always be at least one doctor for every slot. Two doctors request sick leave at the same time for the same slot. Query checks that there are two doctors for this slot and both go off.

#### Characterizing write skew

**Write skew** - generalization of lost update problem (lost update happens to the same record, while write skew is about separate records). It is the anomaly when two **parallel transactions run read queries**, which the further **update logic depend on**, leading to **logic execute in both cases** instead of one.

**Write skew** can happen **if write depends on presence of some records**, while this **write may modify this set of records** on it's own. Hence, if two transactions read the value, first makes the modification, then second is not aware that subset has changed and write skew happens.

Solutions:
- **automatic** write skew prevention requires **serializable isolation level**;
- **DB constraints** that enforce consistency (e.g. PostgreSQL Exclude checks);
- **explicit lock** on rows the transaction depend on.

#### More examples of write skew

- **meeting room booking system** - two users may try to book the same room at the same time;
- **multiplayer game** - two players may try to put the figures on the same position;
- **user registration** (unique email) - both transactions check that username doesn't exist and both try to insert a new user;
- **double-spending** - two transactions check that there's enough money and both spend so that balance becomes negative.

#### Phantoms causing write skew

The write skew pattern is following:
1. select **query** searches for rows that **match some condition**;
2. depending on the result of query, **app decides what to do** (either report an error or make the write);
3. if condition from step 2 is satisfied, app **makes a write and commits** the transaction. The committed **write affects the result of query** from step 1.

**Phantom** - the effect when the **write in one transaction changes the query result in another**.

#### Materializing conflicts

**Materializing conflicts** - **last resort** approach to **prevent write skew** when **write depends on absence of rows**. The **separate table** is introduced, which is not used for storing the actual information, but rather it is **used as a collection of locks**. **Read query will lock respective rows** in this table and other transaction which overlaps with current one will have to wait.

Considering meeting room booking example, table will contain data about possible 15 minutes time slots for every room for next 6 months. The transaction will lock overlapping rows in this table.

This approach is a **Last Resort**, usually **Serializability should be used instead**.

## Serializability

**Serializable** isolation level guarantees that even though **transactions may run in parallel**, the final **result** would be **as if they ran serially**.

**Techniques to implement** serializability:
- literally execute transactions serially;
- two-phase locking (2PL);
- serializale snapshot isolation.

### Actual Serial Execution

#VoltDB , #Redis, #Datomic, #H-Store

Reasons to rethink actual serial execution:
- RAM is cheap, some DBs keep everything in memory;
- OLTP transactions are usually short, while OLAP are usually readonly and can operate on snapshot isolation level.

#### Encapsulating transactions in stored procedures

The **interactive style** of transactions leverages network a lot (requests, responses). If we allow only one transaction to run in one point of time, this would lead to **dreadful performance**.

Another approach is to use **stored procedure** and make the **DB aware of all steps of the transaction** beforehand. Even though transactions are executed one by one they don't wait for the application to submit the next query, hence **performance is NOT limited by network** anymore, also we don't have the concurrency control overhead and locks. If  the **dataset is completely in memory**, we'll have **no disk IO overhead**. This allows us to have pretty good performance in single-threaded environments.

#### Pros and cons of stored procedures

Bad reputation of stored procedures:
- each vendor uses it's **own language** to write them. This is not a general-purpose language, no libraries are available;
- **maintenance is harder** - hard to debug, tricky to test, need to deploy, harder to maintain in VCS, monitoring and metrics collection is not possible;
- **instance performance** - multiple app instances may connect to the single database. While app scaling is relatively simpler, DB scaling is much more harder. Therefore, **poorly written** code in **procedure** leads to **worse outcome** compared to the same poor app code.

Though, **some DBs** now **use general-purpose languages** (VoltDB uses Java and Groovy, Datomic uses Java and Clojure, Redis uses Lua).

#### Partitioning

For apps with **high write throughput** a single-threaded transactions processor may be a **serious bottleneck**.

Write throughput may be **scaled to multiple CPUs** if it is possible for application to partition the data in the way so that **transactions operate within the partition boundaries**.

In VoltDB it is also possible to run **cross-partition transactions**. Though, it's write throughput is quite limited and can't be scaled, as well as it is a lot **slower compared to single-partition transaction**.

#### Summary of serial execution

Actual serial execution is viable with remarks:
- every **transaction must be lightning-fast** (no network communication, everything through stored procedure);
- **active data set must fit in memory**. Some rarely-used data may be backed up on disk, but when it's necessary again, **anti-caching** approach should be used - roll back transaction, fetch data in memory, restart the transaction;
- **write throughput** must be **low enough** to be handled by single CPU, **otherwise partitioning is necessary** to be used with the requirement of no cross-partition transaction;
- **cross-partition transactions are possible** as well, though require DB coordination and are substantially slower compared to single-partition tansactions.

### Two-Phase Locking (2PL)

### Serializable Snapshot Isolation (SSI)

## Summary