# Replication

> The major difference between a thing that might go wrong and a thing that possibly can not go wrong is that when a thing that can not posiibly go wrong goes wrong, it usually turns out to be impossible to get at or repair.

**Replication** - keeping the same **data on multiple machines**, that are connected via a network.

Reasons to replicate data:
- **reduce latency** for users by keeping servers geographically close to them;
- **increase availability** - continue operate even if some parts have failed;
- **increase read throughput** - scale out number of nodes to serve read queries.

Approaches to *replicate changes*: **single-leader**, **multi-leader**, **leaderless**.

## Leaders and Followers

**Replica** - each node that stores copy of data.

Most common way to end up with same data on all replicas is **leader-based  replication** (aka, **active/passive**, **master-slave**):
1. **leader** (master, primary) replica is the first who accepts **write operations**;
2. **followers** (slaves, secondaries, hot standbys) - replicas, which use the **change stream** (repliation log) from *leader* in order **to update local data**;
3. **writes** are served only **by leader**, whereas **reads** may be served either **by leader or followers**.

Support:
- relational: PostgreSQL, Oracle, MySQL, SQL Server;
- non-relational: MongoDB, RethinkDB, Espresso;
- message-brokers: Kafka, RabbitMQ - for highly-available queues.

### Sync VS Async Replication

Usually, the replication is quite fast - *within a second*. However in some cases (network issues, failure recovery, max load) **followers might fall behind** by several minutes.

Sync replication:
- *pros*: the **follower** is guaranteed to be **up-to-date** with the leader. If the leader suddenly fails, all the master data is still available on the follower
- *con*: if the **follower doesn't respond** (crashed, network issue, etc) the write can not be processed.

**Semi-syncronous configuration**. In practice, if replication is made sync, it means that only **one follower is sync**. In case if it becomes slow, one of the async followers is made sync. This guarantees up-to-date data on **at least two nodes**. 

Often the leader-based replication is made **completely asyncronous**. In this case, **writes are not guaranteed**, because if leader fails, any of it's followers may become leader. 
However, **leader** may **process writes** even if all of **followers** have **fallen behind**.

>  **Research on Replication**
>  To not lose data if leader fails, and still have good performance and availability, **chain replication** may be used.
>  **Consensus** (getting several nodes to agree on value) has strong connection to consistency.

### Setting up new Followers

Setting up new follower should be done without downtime.

1. Take consistent data **snapshot from leader**;
2. **Roll out** this snapshot to new follower node;
3. The follower connects to leader and **asks log of all changes** which happened from the time when snapshot was taken (starting pointer is *log sequence number* or *binlog coordinates*);
4. Once follower processed all leader changes, it is called **caught up**.

### Handling node Outages

#### Follower failure: catch-up recovery

If network interrupted, or follower node was restarted, it has to **catch up leader changes**. Follower knows last transaction which it executed from log. On recovery it connects to leader and **asks for all changes** happened during the time when follower was disconnected.

#### Leader failure: failover

**Failover** - process when system has to **promote one** of the followers **to the leader** when such failed. Clients should send their writes to new leader, other followers should start consuming data changes from new leader.

Automatic failover:
1. Determine that **leader has failed** - if node doesn't respond for some time (e.g. 30 seconds), it is considered to be dead;
2. **Choose new leader** - who to chose could be decided with election process (who most replicas agree on), or a leader could be chosen by previously elected *controller node*;
3. **Reconfigure system** to the new leader. Clients have to send requests to the new leader. If old leader comes back, it has to become follower of a new leader.

Pitfalls:
- If a **new leader did not receive all updates** from the old one and old comes back - it's data updates may **conflict with existing data**. In this case, usually such **data is discarded**, which violates durability;
- **Discarding** writes is **especially dangerous** if some **other system** needs to be correlated with DB. Say, redis may store incremental IDs, which were discarded because of failover and new same ID are written later on - this may lead to **information disclosure**.
- In some fault scenarios it is possible that **two nodes consider that they are leaders**. This is called **Split Brain**. If both are accepting writes, and there is no way to resolve conflicts, possibly **data will be lost or corrupted**.
- Too **short timeout** to consider leader dead means that there could be **unnecessary failovers**. Making it **too long** means **longer recovery** in case where leader fails.

There are **no easy solutions** for this kind of problems. That's why some operations teams prefer to **do manual failovers**, even if system supports automatic.

### Implementation of Replication Logs

#### Statement-based replication

In simplest case, leader **logs every single write statement**, and later sends it to followers.

This may break if:
- statements use `RAND()` , `NOW()` and similar **functions**;
- statements use **autoincrementing column**, or **depend on existing data** (like `UPDATE WHERE`), they must be executed  **precisely in the same order** on each replica;
- statements **have side-effects** (triggers, user-defined functions), then each replica may have **different side-effects**.

Generally **other replication methods are preferable**, because there are a lot of edge cases which could not be covered by plain replacement of non-deterministic function-call with static value in the log file.

#### Write-ahead log (WAL) shipping

As the WAL is anyway written to the disk (in LSM-trees and B-Trees), we may send it to the followers as well. Used in Postgres, Oracle.

The problem is that **WALs are low level** (like which bytes were changed in which disk blocks), which makes replication coupled to the storage engine. 

When it is necessary to **upgrade database version**, there are two cases:
- the newer version of storage engine on replicas can't accept old format. In this case upgrade **would require downtime (BC break)**;
- the newer version of storage engine on replicas may accept old WAL format. In this case it is necessary to upgrade all followers first. It can be done **without downtime (BC retained)**.


#### Logical (row-based) log replication

The **dedicated log** for replication is used.  Used in MySql.

Log describes enough row-based data to reproduce events:
- for deletion, primary key is written;
- for creation, all columns data is written to the log;
- for updation, all changed columns are written.

Since replication log is decoupled from WAL, it is easy to **use different versions of database** across the nodes, because replication BC is easily achievable. Moreover, it is even possible to use **different storage engines**.

It is also easier for **separate applications to parse the log**. Therefore this data may be sent to the Data Warehouse or to main app in order to build caches/indices/etc. This technique is called **change data capture**.

#### Trigger-based replication

The most flexible way to replicate the data, though it has the most overhead. It uses **triggers and stored procedures**. 

May be useful in case when it's necessary to **replicate only the part of the data** or if **custom conflict resolution logic** is necessary or for any other **custom things**.

When event happens, *trigger* may **write the changes to the separate table**, which is being somehow **processed by our application**. Databus for Oracle and Bucardo for Postgres works like this.


## Problems with Replication lag

Scaling the read requests makes **synchronous replication almost unusable**, because single node outage would lead to completely blocked writes.

When application **reads from async follower**, the **data may differ** from if it was read **from the leader**. Though, if for example stop all writes, wait a while, then all nodes will become consistent. This is called **eventual consistency**.

### Reading your own writes

There may be the case when shortly after user has submitted the data, he will **see obsolete information** (not seeing the written data), as the data is retrieved from the replica.

The **read-after-write** (aka, **read-your-writes**) consistency is necessary to sort out this problem:

- read the things which **could've possibly been modified from the leader** only (current customer profile information as an example);
- if almost every thing could've been modified, the customer **last modification time** may be kept. Then, **for one minute** after the modification, reads are **served by the leader** only. Also, it is possible to monitor the replication lag and prevent queries to the not caught up followers;
- the **last modification timestamp** may be kept for every customer. The reads are **served from the caught up at least to this timestamp** replicas. If no replicas can handle the read, then query may wait.

The **cross-device** read-after-write consistency - we want to reflect recent modifications on all customer devices (like web app and mobile):
- the customer **last update timestamp must be centralized**, because one device doesn't know if another had any modifications;
- if replicas reside in different datacenters, there's no guarantee that all devices will be connected to the same datacenter.

### Monotonic reads

There may be the case when the same **request is first sent to the fresh replica**, then **to the stale replica**. Things will look like going backward in time. To prevent this, we need **monotonic reads**.

The **monotonic read** - is stronger guarantee than **eventual consistency**, though it is weaker than strong consistency. For any particular user, 
the requests are sent to some particular replica.

### Consistent prefix reads

There may be a **violation of causality** when the data is both **partitioned** and **replicated** and some partitions replicate faster than others. Let's say there's a conversation. If the observer looks on it through the followers, then one participant replies may show up before the actual questions (the question replication lag is far greater than the reply replication lag).

**Consistent prefix reads** - type of guarantee, in which if writes were done in some order, then **reads must appear precisely in the same order**. 
This can be accomplished by enforcing all **causal writes to go into the same partition**.

### Solutions for Replication Lag

It should be anticipated how system has to operate when the replication lag is couple of minutes (possibly, hours). If the *eventual consistency* is enough, - that's great. If not, it would make sense to implement *read-after-write* strategy.

Though, sorting out replication issues on application code level is really complex and easy to get wrong. It would be much simpler if the database could handle such complexity for us. 

The **transactions** were brought in to simplify the application code development. They are usual for single-node database systems. But for distributed systems, some alterative mechanisms may be used (covered in Chapter 7, 9 and part 3)

## Multi-Leader Replication

**Multi-leader** (**master-master**, **active-active**) replication - multiple nodes accept write queries, each of them forwards data to other nodes. Each leader may be a follower to another leader.

### Use cases of multi-leader replication

With such benefits rarely outweigh the complexity, therefore it is not much used.

#### Multi-datacenter operation

Each datacenter may have one leader, and multiple followers. Leaders will synchronize between each other.

- **performance** - since writes go to the local datacenter leader, it is faster than if directed to the single datacenter;
- **tolerance of datacenter outages** - when some datacenter with the leader fails, system will continue it's operation, once it comes back online, replication catches up;
- **tolerance of network problems** - temporary network interruption does not prevent writes from being processed.

Multi-leader tools:
- Tungsten Replicator (MySQL);
- BDR (PostgreSQL);
- GoldenGate (Oracle).

The **main downside** is that multiple leader nodes **may update the same data** - the **write conflicts** must be resolved.

> Auto-incrementing keys, triggers, integrity constraints are problematic with multi-leader replication, therefore this kind of replication **should be avoided if possible**.

#### Clients with offline operation

The client application may have the requirement of offline operation. Then every device will have it's own leader database. Once back online, databases are synchronized. The replication lag may be several hours or even days.

> As the rich history of broken calendar sync shows, multi-leader replication is a hard thing to get right.

CouchDB is designed to make multi-leader configuration easier.

#### Collaborative editing

**Real-time collaborative editing apps** allow multiple users to edit the same document (like Google Docs).

To guarantee that there will be no editing conflicts, the app must **obtain the lock on the document** before the user can edit it. If another user wants to edit, he would need to wait for previous user to commit the changes.

Usually **unit of change** is made very **small** (single keystroke), because this allows faster collaboration and avoid locking. Though, it **requires conflict resolution**.

### Handling write conflicts

The users may **update the same entity at the same time on different leaders**. For instance, first user sets title=B, while second sets title=C. Both of them see successful message. 

#### Sync vs Async conflict detection

The conflict is detected later on asynchronously, and it may be ***too late to ask the user to resolve the conflict***.

If we would stick to sync conflict detection, we will lose the independence of leaders. This is the same as if we had a single leader replication.

#### Conflict avoidance

The recommended approach to deal with conflicts for most of the cases is **to avoid the conflicts**. The particular records are allowed to be edited only from particular leader. 

For example, if application allows user to edit their own profile data, **each user will have "home" datacenter**, where all his writes will come to. It's selection may depend on geographic location.

#### Converging toward a consistent state

The conflicts must be resolved in a **convergent** way - all leaders must eventually have the same value for each of the records.

Ways to achieve convergent conflict resolution:

- give each write an ID (either random number or kty-value hash or uuid or timestamp), and let the **write with the higher ID-number win**. If timestamp is used, it is called LWW (**last write wins**). Approach implies data loss;
- give each leader an ID, and let the **writes from the higher-ID replicas win**;
- **merge values** somehow - like order values, and concatenate;
- record the conflict and **write the application code** to resolve it later on (possibly by prompting the user). 

#### Custom conflict resolution logic

The databases usually provide a way to customize the resolution logic. It may be either **on write** or **on read**.

- **on write** - PostgreSQL Bucardo. When conflict is detected from replication stream, **custom code is executed**.
- **on read** - CouchDB. All the **conflicting writes are stored**. Later on, when data is read, the application can **show user the conflicting data** and let him decide what to do with it.

> Conflicts relate to individual rows (not to the transaction).

##### Automatic conflict resolution
- Conflict-free replicated data types (**CRDTs**) - sets, maps, ordered lists, counters, etc that can be concurrently edited. Can be used in **any topology**;
- **Mergeable persistent data structures** - track all changes explicitly, use three-way merge strategy (compared to CRDTs which use 2-way merge);
- **Operational transformation** - used in Etherpad, Google Docs. Specifically for ordered lists of items. The reliable algorithms require **central server**.

#### What is a conflict

The conflict may be not only **on a row-basis**, but on **a table-basis as well**. For example, if table has constraint that time slots should not overlap. There may be a race condition when two records for the same time slot were booked on the different leader databases.

### Multi-leader Replication Topologies

Topologies:
- **circular** (MySQL default);
- **star** - can be generalized to a tree;
- **al-to-all**;

In **circular** topology writes are propagated along with new writes of nodes, which they were propagated on (IOW, each node appends it's own replication writes).

> To prevent infinite replication loops, each write is tagged with current node identifier. If node receives writes with it's own id, these are just ignored.

If a **one node fails in *circular* or *star* topologies**, it may **interrupt the flow** of replication. Nodes may be reconfigured to communicate around a failed node, but it is mostly done manually.

The fault tolerance of more dense topologies (**all-to-all**) is better, though other **pitfalls of causality** arise.

There may be the case when some **required writes were not processed yet** on a replicated leader, while it **receives dependent writes** (like updates of rows, which were never inserted).

> ? Why won't we just append all writes to the current batch of writes the same way as in circular topology?


## Leaderless Replication

**Leaderless** replication - **each replica accepts writes**, either the application itself or coordinator node sends data to replicas.

### Writing to the database when a node is down

There's **no such thing as failover** in leaderless systems.

The client sends **Quorum Write** to all replicas. Say, if 2/3 databases accepted the write, it is considered successful.

The client sends **Quorum Read** to all replicas as well. The results may differ (some values are up-to-date, others are stale or absent). **Version numbers** are used to **determine latest values**.

#### Read-repair and anti-entropy

**Read-repair** - when the **read** data has **stale values** for some replicas, **newer values are written** to these replicas.

**Anti-entropy process** - the background **process** constantly **checks for differences in the data** and **copies** missing **writes** from one replica to another.

#### Quorums for reading and writing

If there are `N` replicas, every **write must be confirmed by `W` nodes**, 
and **at least `R` nodes must be queried** for each read.

As long as `R + W > N` , the reads are guaranteed to be up-to-date, because in system there may be **no more than `N - W` stale replicas**.

> Usual choice is to set `R = W = (N + 1) / 2`, where `N & 1`. Though, in systems where writes are rare, it would make sense for `W` to approach to `N`, hence reads will be faster.

Fault tolerance:
- if `W < N`, we can tolerate `N - W`  unavailable nodes;
- if `R < N`, we can tolerate `N - R` unavailable nodes.

### Limitations of Quorum consistency

If `W + R <= N`, then **quorum condition** is not satisfied. Though, it allows to reduce latency and increase availability.

There may be edge cases even when **quorum condition is satisfied**, but **stale reads are still possible**:
- a **sloppy quorum** is used - the `W` writes may end up on different nodes, than used by `R` reads;
- on **concurrent writes** - not clear which one happened fist;
- on **race condition between read and write** - write may be reflected only on some replicas;
- if **write has failed** on some of the nodes `W` is not fulfilled, the writes on succeeded nodes is **not rolled back**. Reads may or may not return new data;
- if the **node carrying write** fails, and is restored from stale node, the quorum condition may be **no longer satisfied**.

#### Monitoring staleness

Staleness monitoring for leaderless systems is quite hard. There are almost no tools for it.

### Sloppy quorums and Hinted Handoff

#Dynamo, #Riak, #Cassandra, #Voldemort

If the client faces network interruption, a lot of alive nodes may look like dead to him. Though, for other clients they are still alive.

**Sloppy quorum** - when `R` or `W` can't be reached on `N` designated nodes, the **requests are sent to other nodes**, which are **not in the list** of _designated_.

**Hinted handoff** - once issue with `N` main nodes is fixed, **sloppy quorum writes** are sent to their **home nodes** (`N`).

> Sloppy quorums does not guarantee up-to-date read (even when `W + R >N`), because writes may've been **sent to the neighbour nodes** (outside of `N`). Hence, sloppy quorum is not a quorum anymore - it is just durability assurance that values are written somewhere.

#### Multi-datacenter operation

#Cassandra and #Voldemort provide next model for **multi-datacenter replication**:

The `N` is selected among nodes in all datacenters. In configuration we select **how many nodes to wait for each datacenter**.

Usual case is when reads and **writes wait for local datacenter replicas**, because it's not affected by cross-datacenter network issues. The high-latency writes to other datacenters are usually configured to happen in async manner.

### Detecting concurrent writes

When two clients concurrently modify the same piece of data, some nodes may **receive updates in the different order**. 
- Node1 may receive update from client A, and not receive from B (due to network issue);
- Node2 may receive first update from A, then from B;
- Node3 may receive first update from B, then from A.

#### Last write wins (discarding concurrent writes)

In #Cassandra this is the only conflict resolution method.
In #Riak this is optional method.

#LWW -  each write is associated with the timestamp. On conflict, writes 
with more **recent timestamp wins**. 

There may even be the cases when **LWW drops writes** which are **not concurrent**. Therefore, it should only be used where **lost writes are acceptable** (like caching).

> This method achieves **convergence** by the **cost of durability** - clients were informed that write was successful, though it is silently discarded later on.

#### The "happens-before" relationship and concurrency

#causality

If operation B **knows about** A or if it **depends on** A, or **builds upon** A in some way, then B is **causally dependent** on A. From other side, event A **happens before** B.

**Concurrency** - if **neither** of operations **happens before** the other (if **neither knows** about the other).  On **concurrent** modifications the **conflicts must be resolved**. 

If **operations are dependent** on each other (no concurrency), then **latter overwrite** former.

#### Capturing the happens-before relationship



