# Partitioning

>> Topics discussed: splitting, indexing, rebalancing, routing

**Sharding** - breaking the data into **partitions**.
**Partition** - the same as **shard**, **region**, **tablet**, **vnode**, **vbucket**. Each partition is a small database on it's own, which holds some subset of records.

Main reason for partitioning is **scalability**.

## Partitioning and Replication

The **replication** may be applied **along with partitioning**. 
It's not necessary that single node holds data of exactly one partition. 
When there are **multiple nodes**, each of them may be a **leader for one partition**, and a **follower for another**.

## Partitioning of key-value data

**Skewed partitioning**  - when some partitions **have more data or queries** than others.
**Hot spot** - partition, which **receives disproportionally high load** compared to other partitions.

### Partitioning by Key Range

Each partition holds values for a **range of keys**. 

We can definitely **know which partition holds which key** since we know the range of each partition.

Partition **boundaries** need to **adapt to the data**, not to the amount of possible keys. For example, in the dictionary there are a lot less words starting from X and Z than from A and B. 

**Boundaries** may be **chosen manually** by admin or **automatically**.
The **keys** inside partition **are sorted** in order to make **easy range scans**.

It is allowed to **partition data by multiple keys**. It's important to select the **first element of the key** such field, which will allow to **spread write load** across partitions.

For instance, when sensor name and timestamp are used as keys, one single partition would correspond to one type of sensors for one specific day (keys inside may hold different sensor names and different timestamps).

### Partitioning by Hash of Key

Easier **to avoid skews** and **hot spots**.

Partitioning is **done by range of hashes** of keys **rather than keys** themselves. The **hash ranges** can be **evenly spaced** as far as data is evenly distributed by the **hash function**.

**Hash function** is used to determine the partition for given key. **Built-in functions** `Object.hashCode()` are **not suitable** for partitioning, as hash may differ in different processess.

> >**Consistent hashing** doesn't work well for databases, hence rarely used.

As far as **data is scattered** across the partitions, it is **not possible** to run effecient **range queries**, which are possible when partitioning by key:
- MongoDB sends range query **to all partitions**;
- Voldemort, Riak, Couchbase **do not support**;
- **Cassandra** does the compromise using **concatenated index**. 

**Concatenated index approach** - elegant way for **one-to-many** relationships partitioning. The many-side table may declare **composite primary key**. **Partitioning** is done only **by the first column**, which is the owner column. This approach allows us to **effectively scan over the many side** of a relationship.

### Skewed Workloads and Relieving Hot Spots

Even with partitioning by hash of key, there may be **extreme hot spot cases** when reads and writes are **for the same key** (e.g. some celebrity).

To **distribute the workload** there's **no easy solution**. The only way to handle it is to write custom **application code**, which will **reduce the skew**. 

For instance, we could add two-digit random number at the beginning of the key in order to split the writes evenly across 100 partitions. Though, for reads an additional logic is required as well since we'd need to read the data from 100 partitions.

It only makes sense to **implement** such logic **for specific set of skewed keys**, not to all of them.

## Partitioning and Secondary Indexes

Since **secondary indexes do not uniquely identify** records, it becomes **complicated** really fast, as we don't know neither **which partition to look** in nor **where record is** located **within the partition**.

>> Search servers ElasticSearch, Solr have their main purpose to implement secondary indexes.

Approaches (schemes) to **partition database with secondary indexes**:
- **document-based** partitioning;
- **term-based** partitioning.

### Partitioning Secondary Indexes by Document

**Local index** - each partition additionally **holds secondary index of its own documents** besides of the data itself.

> If DB **supports only key-value** storage, it is dangerous to implement custom secondary indexes. A **great care** is required to make them **consistent** with the **underlying data**. Issues: **race conditions**, intermittent **write failures** easily cause data get out of sync.

**Scatter-gather** - secondary index read approach which implies **querying all partitions for secondary index** to get list of used documents. It is prone to the **tail latency amplification**. Nonetheless, used in #Cassandra , #VoltDB, #MongoDB, #Riak , #ElasticSearch, #SolrCloud.

The **benefit** is that **writes are still performant**, since secondary index will be maintained **per partition basis**.

The **drawback** is **worse read performance**, sincce secondary index has to be analyzed **on all partitions**.

### Partitioning Secondary Indexes by Term

**Global index** - as opposed to **local index**, covers index data for all partitions. Usually it is **term-partitioned**.

**Term-partitioned index** - such secondary index, in which the **term determines partition** of stored value references. 

**Term partitioning** may be done **by terms themselves** (allows range scans), or **hashes of terms** (better distributes the load).

The **benefit** is that **reads are performant**, since it's necessary to scan only partitions which hold needed secondary index terms.

The **drawback** is that **writes are slower and complicated**. For **index to be up to date**, it should reflect changes to underlying data. Since it is **stored on separate partitions**, it brings **complexity of distributed transactions**.

>> **In practice**, often global secondary indexes **updates are async**. #Dynamo does this way. It is very **prone to the infrastructrue** reliability.

## Rebalancing Partitions

**Rebalancing** - migrating data and moving load from one node to another.

Necessary when:
- **query throughput** increases;
- **data amount** increases;
- **machine fails** - other need to take over.

Rebalancing **requirements**:
- the **bare minimum of data** has to be moved for **fast rebalancing**;
- **during the rebalancing** system has to **accept reads and writes**;
- **after rebalancing** the **load should be fairly shared** between nodes.

### Partition Strategies for Rebalancing

#### Issue with Hash mod N

When using **partitioning by the hash of key**, we may be tempted to use `mod` operation **to decide which partition** is appropriate.

This **approach** makes **rebalancing _extremely expensive_**, since if we add a single node, then we'd need to rebalance a lot of data.

That's why when **partitioning by the hash of key**, it's better to **use hash ranges**, though it is also possible to make **fixed number of partitions** beforehand.

#### Fixed number of partitions

#Riak , #ElasticSearch , #Voldemort , #CouchBase use this approach.

**By the hash of key** we decide **which partition** to forward the request to, **not which node**. 

_Simple solution_ is to **create a lof of partitions** and place them on raletively **small amount of nodes**. When it is time to rebalance, a **new node** will be **brought in**, and some **partitions will move** to that node.

**During data movement** to the new nodes **old partitions are used** for reads and writes.

For instance, if before rebalancing there are 4 nodes with 5 partitions on each, after rebalancing there would be 5 nodes with 4 partitions in each.

It's necessary to **anticipate system growth** from an outset. In future, it would be possible to have **as much nodes, as partitions** there currently are. The number should be **big enough**, but not too big, as far as **each partition** has it's own **management overhead**.

#### Dynamic partitioning

#HBase, #RethinkDB, #MongoDB 

Applicable for both key-range partitioning and hash-partitioning.

If DB uses **key-range partitioning**, it may be the case when only **first partition is used**, while **others are idle**. DB may **create partitions dynamically**. When partition is **big enough, it is split**.

**Advantage** is that **number of partitions** adapts to the **amount of data**. This means, that if not set, initially DB will have a **single partition on a single node**, while **other nodes sit idle**. This can be solved with **pre-splitting**.

**Pre-splitting** - configuration of **initial set of partitions** on empty database.

#### Partitioning proportionally to nodes

#Cassandra , #Ketama

With both **dynamic partitioning** and having **fixed number of partitions**, the **number of nodes** and **number of partitions** are independent.

**Proportional partitioning** - the way to partition the data so that a **static number of partitions per node** is kept. Applicable for **hash-based partitioning only**.

The existing **partitions grow along with the dataset**. Once they can't grow anymore, we **add more nodes**, and **existing partitions become smaller** again. When adding new node **system randomly selects N** (number of partitions per node) **existing partitions**, splits them and **migrates the half of the data** to the new node.

### Operations: Automatic or Manual Rebalancing

**Rebalancing** is an **expensive operation**, because it requires a lot of data migration, which **increases the load on the network and on the nodes** and requires **requests rerouting**. 

Fully **automated rebalancing is unpredictable**. 

If it will happen during system peak time, it may lay down the whole system. When one node is overloaded and slows down to respond, other nodes may think it is dead and move load away from it, in turn slowing down other nodes.  As the outcome we may have cascade failure.

CouchBase, Riak, Voldemort can generate suggested partition-node assignment automatically, but **require administrator to apply it**. That's the best trade-off between automatic and manual rebalancing.

## Request Routing

As far as **keys ownership** or **assignment** of partitions to nodes **may change**, clients need to know **where to send the request to**. This is a **service discovery problem**.

Options:
- clients may **send** the request **to any node** (random or selected by round-robin) and the **node will forward the request** to an authoritative node;
- the **routing tier** may be used as the **proxy to any node**. It will **forward the request** to respective node;
- **clients may know about partitioning** and send the requests to the correct node right away.

In any case, **responsible component must be notified** about any changes in the partitioning scheme: 
1. The **third-party coordination service** (like **ZooKeeper**) may be used. Every node is registered in it and **on any change** the **service notifies responsible component**. The most popular approach, used in HBase, SolrCloud, Kafka, MongoDB (similar approach with own config);
2. Another approach is usage of **gossip protocol**. It **avoids the coordinator service** and makes every **node aware of the routing scheme**. Used in Cassandra and Riak;
3. Not rebalance automaticcaly. Used in Couchbase.

### Parallel Query Execution

**Massively parallel processing** - queries which significantly benefit from the database split, because **queries may execute in parallel** on multiple nodes. Usually such queries are used for [[3. Storage and retrieval#Transaction Processing or Analytics|analytics]] and include buch of joins, aggregations, conditions etc.

## Summary

Partitioning is applied when single node can **no longer store all the data** or **handle all the load**. 

The goal of partitioning is to **split the large dataset** into smaller subsets, avoiding hot spots. To do it, appropriate partitioning scheme must be used.

**Partitioning Approaches**:
- **key-range** partitioning - effecient **range queries** (keys are sorted). **Hot spots** may arise when adjacent keys are highly accessed. Ususally **automatically rebalanced** (dynamic partitioning) by partitions split;
- **hash** partitioning - hashes of keys are used. Evenly **distributed data and load**.  **No range queries** can run effectively. For **rebalancing** ususally **fixed number of partitions** is set from the outset; **dynamic partitioning** may be used as well;
- hybrid with usage of **compound primary key** - part of key split data by partitions, another used for within-partition keys ordering.

**Secondary indexes**:
- **document-partitioned** (local index) - each **partition** maintains it's **own secondary index**. On write **single partition is updated**. On **read scatter-gather** approach is used;
- **term-partitioned** (global index) - secondary index is partitioned **separately from the main data**, but usually stored on the same nodes. **On read, single partition** is searched. **On write, multiple nodes** must accept updates.



