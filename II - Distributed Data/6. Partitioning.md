# Partitioning

>> We must state relationships not procedures.

**Sharding** - breaking the data into **partitions**.
**Partition** - the same as **shard**, **region**, **tablet**, **vnode**, **vbucket**. Each partition is a small database on it's own, which holds some subset of records.

Main reason for partitioning is **scalability**.

## Partitioning and Replication

The **replication** may be applied **along with partitioning**. 
It's not necessary that single node holds data of exactly one partition. 
When there are **multiple nodes**, each of them may be a **leader for one partition**, and a **follower for another**.

## Partitioning of key-value data

**Skewed partitioning**  - when some partitions **have more data or queries** than others.
**Hot spot** - partition, which **receives disproportionally high load** compared to other partitions.

### Partitioning by Key Range

Each partition holds values for a **range of keys**. 

We can definitely **know which partition holds which key** since we know the range of each partition.

Partition **boundaries** need to **adapt to the data**, not to the amount of possible keys. For example, in the dictionary there are a lot less words starting from X and Z than from A and B. 

**Boundaries** may be **chosen manually** by admin or **automatically**.
The **keys** inside partition **are sorted** in order to make **easy range scans**.

It is allowed to **partition data by multiple keys**. It's important to select the **first element of the key** such field, which will allow to **spread write load** across partitions.

For instance, when sensor name and timestamp are used as keys, one single partition would correspond to one type of sensors for one specific day (keys inside may hold different sensor names and different timestamps).

### Partitioning by Hash of Key

Easier **to avoid skews** and **hot spots**.

Partitioning is **done by range of hashes** of keys **rather than keys** themselves. The **hash ranges** can be **evenly spaced**.

**Hash function** is used to determine the partition for given key. **Built-in functions** `Object.hashCode()` are **not suitable** for partitioning, as hash may differ in different processess.

> >**Consistent hashing** doesn't work well for databases, hence rarely used.

As far as **data is scattered** across the partitions, it is **not possible** to run effecient **range queries**, which are possible when partitioning by key:
- MongoDB sends range query **to all partitions**;
- Voldemort, Riak, Couchbase **do not support**;
- **Cassandra** does the compromise using **concatenated index**. 

**Concatenated index approach** - elegant way for **one-to-many** relationships partitioning. The many-side table may declare **composite primary key**. **Partitioning** is done only **by the first column**, which is the owner column. This approach allows us to **effectively scan over the many side** of a relationship.

### Skewed Workloads and Relieving Hot Spots

Even with partitioning by hash of key, there may be **extreme hot spot cases** when reads and writes are **for the same key** (e.g. some celebrity).

To **distribute the workload** there's **no easy solution**. The only way to handle it is to write custom **application code**, which will **reduce the skew**. 

For instance, we could add two-digit random number at the beginning of the key in order to split the writes evenly across 100 partitions. Though, for reads an additional logic is required as well since we'd need to read the data from 100 partitions.

It only makes sense to **implement** such logic **for specific set of skewed keys**, not to all of them.

## Partitioning and Secondary Indexes

Since **secondary indexes do not uniquely identify** records, it becomes **complicated** really fast, as we don't know neither **which partition to look** in nor where record is located **within the partition**.

>> Search servers ElasticSearch, Solr have their main purpose to implement secondary indexes.

Approaches (schemes) to **partition database with secondary indexes**:
- **document-based** partitioning;
- **term-based** partitioning.

### Partitioning Secondary Indexes by Document

**Local index** - each partition additionally **holds secondary index of its own documents** besides of the data itself.

>> If DB **supports only key-value** storage, it is dangerous to implement custom secondary indexes. A **great care** is required to make them **consistent** with the **underlying data**. Issues: **race conditions**, intermittent **write failures** easily cause data get out of sync.

**Scatter-gather** - secondary index read approach which implies **querying all partitions for secondary index** to get list of used documents. It is prone to the **tail latency amplification**. Nonetheless, used in #Cassandra , #VoltDB, #MongoDB, #Riak , #ElasticSearch, #SolrCloud.

The **benefit** is that **writes are still performant**, since secondary index will be maintained **per partition basis**.

The **drawback** is **worse read performance**, sincce secondary index has to be analyzed **on all partitions**.

### Partitioning Secondary Indexes by Term

**Global index** - as opposed to **local index**, covers index data for all partitions. Usually it is **term-partitioned**.

**Term-partitioned index** - such secondary index, in which the **term determines partition** of stored value references. 

**Term partitioning** may be done **by terms themselves**, or **hashes of terms**. First option allows range scans, second better distributes the load.

The **benefit** is that **reads are performant**, since it's necessary to scan only partitions which hold needed secondary index terms.

The **drawback** is that **writes are slower and complicated**. For index to be up to date, it should reflect changes to underlying data. Since it is stored on separate partitions, it brings **complexity of distributed transactions**.

>> **In practice**, often global secondary indexes **updates are async**. #Dynamo does this way. It is very **prone to the infrastructrue** reliability.

## Rebalancing Partitions

**Rebalancing** - migrating data and moving load from one node to another.

Necessary when:
- **query throughput** increases;
- **data amount** increases;
- **machine fails** - other need to take over.

Rebalancing **requirements**:
- the **bare minimum of data** has to be moved for **fast rebalancing**;
- **during the rebalancing** system has to **accept reads and writes**;
- **after rebalancing** the **load should be fairly shared** between nodes.

### Strategies for Rebalancing



### Operations: Automatic or Manual Rebalancing


## Request routing


splitting, indexing, rebalancing, routing
------


